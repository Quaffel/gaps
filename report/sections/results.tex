\chapter{Experiments and results}
To evaluate the performance of the algorithms, we conducted experiments on different board sizes ranging from $4 \times 5$ to $4 \times 13$, with only the number of columns varying. For each board size, we considered games of increasing complexity: 20, 25, 30, and 35. For each level of complexity, we generated 5 games and ran each algorithm on each game. This resulted in a total of:
\begin{align*}
(13-5+1) \cdot 4 \cdot 5 = 180 \text{ games}
\end{align*}
With 3 algorithms to test (Greedy BFS, A$^*$, and MCTS), we had a total of:
\begin{align*}
  180 \cdot 3 = 540 \text{ game instances to solve}
\end{align*}
For each run, we measured the following metrics:
\begin{enumerate}
  \item \textbf{Time taken to solve the game}: This metric helps in understanding the computational efficiency of each algorithm.
  \item \textbf{Path length to a possible solution}: This provides insight into the optimality of the solution found by each algorithm.
  \item \textbf{Success rate}: This indicates the robustness and reliability of each algorithm in finding a solution.
\end{enumerate}
The experimental results will help us compare the performance of our different algorithms, Greedy BFS and A$^*$ and the stochastic algorithm MCTS, particularly in terms of efficiency, and solution quality.

For MCTS we chose to perform 1,000 iterations for each game, as this value was found to be a good trade-off between the time taken to solve the game and the quality of the solution found. We also experimentally chose to set the exploration parameter to $W_c = 0.3$ and the weight of the heuristic to $W_S = 0.2$, these provided good results be can still be adjusted to improve the performance of the algorithm.

For every game dimension and complexity level, we set an increasing timeout in second to avoid the algorithm to run indefinitely, this timeout is defined from the following formula:
\begin{align*}
  t(C, N) = 8(C-4) + 0.7(N-20)
\end{align*}
This ranges from 8 seconds for the simplest game to 82.5 seconds for the most complex game.

\section{Reproducibility}
To ensure reproducibility, we implemented a simple linear congruential generator (LCG) to generate random numbers given a seed. This seed initializes the random number generator, ensuring that the same sequence of random numbers is generated each time the algorithm is run with the same seed. This approach allows us to reproduce the same results consistently. The LCG is used whenever random numbers are needed in our algorithms, such as in the shuffling of the game and in the MCTS algorithm.

Each game we generate has an associated seed, enabling the reproduction of the same game configuration. An identical board can be generated by using the same number of rows $R$, columns $C$, the number of shuffling steps $N$ and seed.

Greedy BFS and A$^*$ are deterministic algorithms, meaning they will always find the same solution if given the same initial state. On the other hand, MCTS is a stochastic algorithm, involving some randomness in the decision-making process. To allow for reproducibility in MCTS, we used the LCG with a seed that depends on a default value plus the current iteration number. This approach ensures that different actions are taken in each iteration, while still allowing us to reproduce the same results for MCTS.

The code which contains the seed and all the parameters used to generate the game is available in the \texttt{src/headless.ts} file. The script can be run with the following command: \texttt{npm run headless}, the output of the experiment will be in the file \texttt{dist/results.csv} and can be analysed with the Jupyter Notebook \texttt{plot.ipynb}.

\section{Justification of the weight choice}
The weights and scoring methods for evaluating game states were chosen after thorough experimentation and analysis. We first identified the crucial factors influencing the game's solvability, such as the number of well-placed cards and the presence of problematic gaps. Each factor was assigned a weight based on its importance in achieving the game's objective, with higher weights given to criteria that directly contribute to successful solutions.

The weights were then fine-tuned by simulating different game scenarios and observing how various scoring combinations impacted the MCTS and A* algorithms' performance. With repeated adjustments, we developed a balanced scoring system that prioritizes well-placed cards while minimizing the effect of challenging gaps. This setup enables the algorithm to consistently select good moves for solving the Gaps card game.

\section{Results by size}
\begin{table}[H]
\centering
\input{results_table_1.tex}
\caption{"Size" is the number of cell in the board ($R \times C$), the columns "Path length" and "Time elapsed (s)" display their average values for the games that could have been solved. Not doing so would average the timeout values and a path length of 1 for the games that could not be solved, which would not be representative of the actual performance of the algorithm.}
\end{table}

\section{Overall results}
\begin{table}[H]
  \centering
  \input{results_table_2.tex}
  \caption{"Path length" and "Time elapsed (s)" display their average values for the games that could have been solved. Not doing so would average the timeout values and a path length of 1 for the games that could not be solved, which would not be representative of the actual performance of the algorithm.}
\end{table}

\newpage
\section{Performance by size}
\figureWithCaption{time_elapsed_by_size}{}{13cm}
\figureWithCaption{path_length_by_size}{}{13cm}
\figureWithCaption{success_rate_by_size}{}{13cm}

\newpage
\subsection{Overall performance}
\figureWithCaption{overall_performances}{Overall performance}{\columnwidth}
